{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "decision_tree.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv0t7rgxaTIY"
      },
      "source": [
        "import sys\n",
        "import csv\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from numpy import *\n",
        "# import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tdl1dqyaTId"
      },
      "source": [
        "## Multiclass classifier decision tree using ID3 algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmwIa5RraTId"
      },
      "source": [
        "#normalize the entire dataset prior to learning using min-max normalization \n",
        "def normalize(matrix):\n",
        "#     transfer the data metrix to np array in float type.\n",
        "    a = np.asarray(matrix)\n",
        "    #print(a[:,-1])\n",
        "    a = a.astype(float)\n",
        "    #print(\"Before normalizing\")\n",
        "    #print(a)\n",
        "#     apply the normalization along the 0 axis of a using the formula: (x - x_min)/(x_max - x_min)\n",
        "    a = (a - a.min(axis=0))/(a.max(axis=0) - a.min(axis=0))\n",
        "    #print(\"After normalizing\")\n",
        "    #print(a)\n",
        "    return a\n",
        "#matrix = load_csv('iris.csv')\n",
        "#normalize(matrix[:,:4])\n",
        "#normalize(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prA6fQoTaTIe"
      },
      "source": [
        "# reading from the file using numpy genfromtxt with delimiter ','\n",
        "def load_csv(file):\n",
        "    Z = np.genfromtxt(file, delimiter=',', dtype = str)\n",
        "    return (Z)\n",
        "\n",
        "# Z = load_csv(\"iris.csv\")\n",
        "# Z[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izdwY4r2yS70"
      },
      "source": [
        "# reading from the file using numpy genfromtxt with delimiter ','\n",
        "def load_csv2(file):\n",
        "    Z = np.genfromtxt(file, delimiter=',', dtype = str, max_rows = 400, skip_header = 1600)\n",
        "    return (Z)\n",
        "\n",
        "# Z = load_csv(\"spambase.csv\")\n",
        "# Z[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMlOwaAaTIe"
      },
      "source": [
        "#method to randomly shuffle the array using the numpy.random.shuffle()\n",
        "def random_numpy_array(ar):\n",
        "    np.random.shuffle(ar)\n",
        "    return ar\n",
        "#print(random_numpy_array(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0lPDwaHaTIe"
      },
      "source": [
        "#Normalize the data and generate the training labels,training features, test labels and test training\n",
        "def generate_set(X):\n",
        "    #print(X.shape[0])\n",
        "#     store the label X[:,-1] to Y\n",
        "    Y = X[:,-1]\n",
        "    #print(Y)\n",
        "#     reshape y to (Y's length, 1)\n",
        "    Y = Y.reshape(len(Y),1)\n",
        "#     store it to j\n",
        "    j = Y\n",
        "\n",
        "    # print(\"J is\",j)\n",
        "#     create the new_X which exclude the label X[:,:-1]\n",
        "    new_X = X[:,:-1]\n",
        "\n",
        "#     normalize the data step\n",
        "#     using our implemented function normalize()\n",
        "    norm_X = normalize(new_X)\n",
        "\n",
        "#     add the label back to the normiazlied X\n",
        "#     using np.concatenate along axis=1\n",
        "    X = np.concatenate((norm_X,j),axis=1)\n",
        "\n",
        "#     store the size of rows of the normalized X with labels\n",
        "    rows_X = X.shape[0]\n",
        "\n",
        "#     use the 10% of the data to be the test set.\n",
        "#     store the number of testing data\n",
        "    num_test = math.floor(rows_X * 0.1)\n",
        "\n",
        "#     set the starting idex to be 0\n",
        "    start_idx = 0\n",
        "\n",
        "#     set the ending index to be the number of testing data\n",
        "    end_idx = num_test\n",
        "#     create a list that store all features of the testing data\n",
        "    test_features = []\n",
        "#     create a list that store all labels of the testing data\n",
        "    test_labels = []\n",
        "#     create a list that store all features of the training data\n",
        "    train_features = []\n",
        "#     create a list that store all labels of the training data\n",
        "    train_labels = []\n",
        "#     10-fold cross-validation:\n",
        "    for i in range(10):\n",
        "#         store the test set for corss-validation using X[start:end,:]\n",
        "      test = X[start_idx:end_idx,:]\n",
        "#         get training data before the testing data X[:start, :]\n",
        "      train_before_test = X[:start_idx, :]\n",
        "#         get training data after the testing data X[:start, :]\n",
        "      train_after_test = X[end_idx:, :]\n",
        "#         form the new training set using np.concatenate\n",
        "      train = np.concatenate((train_before_test, train_after_test))\n",
        "#         get the testing set labels\n",
        "      test_set_labels = test[:,-1]\n",
        "#         flattent the labels\n",
        "      test_set_labels.flatten()\n",
        "#         get the training set labels\n",
        "      train_set_labels = train[:,-1]\n",
        "#         flattent the labels\n",
        "      train_set_labels.flatten()\n",
        "#         create the test set exclude the labels\n",
        "      test = test[:,:-1]\n",
        "      test = test.astype(np.float)\n",
        "#         same for the training set\n",
        "      train = train[:,:-1]\n",
        "      train = train.astype(np.float)\n",
        "\n",
        "#         append test data of this fold to the list\n",
        "      test_features.append(test)\n",
        "#         append test lables of this fold to the list\n",
        "      test_labels.append(test_set_labels)\n",
        "#         do the same for the training set\n",
        "      train_features.append(train)\n",
        "      train_labels.append(train_set_labels)\n",
        "\n",
        "#         update the index pointer\n",
        "      start_idx = end_idx\n",
        "      end_idx += num_test\n",
        "\n",
        "#     return the fold list that contain data and label for both training and testing set.\n",
        "    return train_features, train_labels, test_features, test_labels\n",
        "\n",
        "# matrix = load_csv('iris.csv')\n",
        "# generate_set(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTBAePwKaTIf"
      },
      "source": [
        "#build a dictionary where the key is the class label and values are the features which belong to that class.\n",
        "def build_dict_of_attributes_with_class_values(X,y):\n",
        "#     init a dict for attributes\n",
        "    dic = {} \n",
        "#     init feature list.\n",
        "    feature = []\n",
        "#     for each feature in the dataset\n",
        "    #print(X)\n",
        "    for i in range(X.shape[1]-1):\n",
        "#         store the featur index\n",
        "        feature_idx = i\n",
        "#     find all the value correspond to this feature\n",
        "        val = X[:,i]\n",
        "        #print(val)\n",
        "#     init an attribute list\n",
        "        feature_attribute = []\n",
        "#     init the counter to 0\n",
        "        count = 0\n",
        "#         for each value in the \"all the value correspond to this feature\"\n",
        "        for j in val:\n",
        "#             init a empty list that store the attribute value\n",
        "            val_attribute = []\n",
        "#             append the this value to the list\n",
        "            val_attribute.append(j)\n",
        "#             append the label of this value to the list\n",
        "            val_attribute.append(y[count])\n",
        "#             append this list to the attribute list.\n",
        "            feature_attribute.append(val_attribute)\n",
        "#             increase the counter\n",
        "            count += 1\n",
        "#         add this attribute list to the dict according to the feature index\n",
        "        dic[feature_idx] = feature_attribute\n",
        "#         append the feature indx to the feature list.\n",
        "        feature.append(feature_idx)\n",
        "#     return the dict and feature list.\n",
        "    # print(\"dict: \", dic)\n",
        "    # print(\"dict len\", len(dic), len(dic[0]))\n",
        "    return dic, feature\n",
        "\n",
        "# X = load_csv(\"iris.csv\")\n",
        "# y = X[:,-1]\n",
        "# build_dict_of_attributes_with_class_values(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnBTBwBEaTIf"
      },
      "source": [
        "# Iterative Dichotomiser 3 entropy calculation\n",
        "def entropy(y):\n",
        "#     init a class frequence dict\n",
        "    class_freq = {}\n",
        "#     init the attribute entropy to 0\n",
        "    attribute_entropy = 0\n",
        "#     for each label in y:\n",
        "    for i in y:\n",
        "#         this is label is already in the dict, we increase its feq\n",
        "        if i in class_freq:\n",
        "            class_freq[i] += 1\n",
        "\n",
        "#          else, we set the freq to 1\n",
        "        else:\n",
        "            class_freq[i] = 1\n",
        "\n",
        "#     calculate the cumulate entropy using the formula.\n",
        "    for i in class_freq:\n",
        "        attribute_entropy += -(math.log(class_freq[i]/sum(class_freq[i] for i in class_freq)) * (class_freq[i]/sum(class_freq[i] for i in class_freq)))\n",
        "    return attribute_entropy\n",
        "\n",
        "#entropy(Z[:,-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sNOyj0oaTIf"
      },
      "source": [
        "#Class node and explanation is self explaination\n",
        "class Node(object):\n",
        "#     init the node with val,lchild,rchild,theta and leaf.\n",
        "    def __init__(self, val, lchild, rchild,theta,leaf):\n",
        "        self.root_value = val\n",
        "        self.root_left = lchild\n",
        "        self.root_right = rchild\n",
        "        self.theta = theta\n",
        "        self.leaf = leaf\n",
        "\n",
        "#     method to identify if the node is leaf\n",
        "    def is_leaf(self):\n",
        "        return self.leaf\n",
        "\n",
        "#     method to return threshold value\n",
        "    def ret_thetha(self):\n",
        "        return self.theta\n",
        "    \n",
        "#     method return root value\n",
        "    def ret_root_value(self):\n",
        "        return self.root_value \n",
        "    \n",
        "#     method return left tree\n",
        "    def ret_llist(self):\n",
        "        return self.root_left\n",
        "\n",
        "#     method return right tree\n",
        "    def ret_rlist(self):\n",
        "        return self.root_right\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"(%r, %r, %r, %r)\" %(self.root_value,self.root_left,self.root_right,self.theta)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDsfZz6SaTIg"
      },
      "source": [
        "\n",
        "#Decision tree object\n",
        "class DecisionTree(object):\n",
        "#     init a variable called fea_list\n",
        "    fea_list = []\n",
        "#     init the Dtree by setting the root node to None\n",
        "    def __init__(self):\n",
        "        self.root_node = None\n",
        "\n",
        "    #method to return the major class value using Counter() and .most_common()\n",
        "    def cal_major_class_values(self,class_values):\n",
        "        major_class_values = Counter(class_values).most_common(1)[0][0]\n",
        "        return major_class_values\n",
        "\n",
        "    #method to calculate best threshold value for each feature\n",
        "    def cal_best_theta_value(self,ke,attri_list):\n",
        "#         init a list for data\n",
        "        # print(attri_list)\n",
        "        data = []\n",
        "#         init a list for class labes\n",
        "        class_labels = []\n",
        "#         for each attribute in the attri_list=\n",
        "        for i in attri_list:\n",
        "#             append the data\n",
        "            #print(i[0])\n",
        "            data.append(i[0])\n",
        "#             append the feature value.\n",
        "            class_labels.append(i[1])\n",
        "#         calculate the entropy of those feaure values\n",
        "        # data = [x.astype(np.float) for x in data]\n",
        "        entropy_feature_values = entropy(class_labels)\n",
        "#         init the max info gain = 0\n",
        "        max_info_gain = 0\n",
        "#         init theta=0\n",
        "        theta = 0\n",
        "#         init a list that store the best index on the left\n",
        "        best_idx_left = []\n",
        "#         init a list that store the best index on the right\n",
        "        best_idx_right = []\n",
        "#         init a list that store class labels after split\n",
        "        class_labels_split = []\n",
        "#         sort the data\n",
        "        data.sort()\n",
        "#         for each index of data:\n",
        "\n",
        "        now = 0\n",
        "        after = now + 1\n",
        "        for i in range(len(data)-1):\n",
        "#             calculate the current theta using data[i]+data[i+1])/ 2\n",
        "        # print(\"data[i]: \", data[i])\n",
        "        # print(\"type - data[i]: \", type(data[i]))\n",
        "        # print(\"type - data[i+1]: \", type(data[i+1]))\n",
        "        # print(\"data[i] + data[i+1]: \", data[i]+data[i+1])\n",
        "            if data[now] != data[after]:\n",
        "                current_theta = (float(data[now]) + float(data[after])) / 2.0\n",
        "                now = after\n",
        "                after += 1\n",
        "            else:\n",
        "                after += 1\n",
        "                continue\n",
        "\n",
        "  #             init a list that store index that less than theta\n",
        "            idx_less_than_theta = []\n",
        "  #             init a list that store value that less than theta\n",
        "            val_less_than_theta = []\n",
        "  #             init a list that store index that greater than theta\n",
        "            idx_greater_than_theta = []\n",
        "  #             init a list that store value that less than theta\n",
        "            val_greater_than_theta = []\n",
        "  #             init the counter to 0\n",
        "            count = 0\n",
        "  #             for each index and value in attri_list\n",
        "            for c,j in enumerate(attri_list):\n",
        "  #                 if value less or equal than the current theta:\n",
        "                if (float(j[0]) <= float(current_theta)):\n",
        "  #                     update the \"less\" list of index and value\n",
        "                    idx_less_than_theta.append(c)\n",
        "                    val_less_than_theta.append(j[1])\n",
        "  #                 else:\n",
        "                else:\n",
        "  #                     update the \"greater\" list of index and value\n",
        "                    idx_greater_than_theta.append(c)\n",
        "                    val_greater_than_theta.append(j[1])\n",
        "\n",
        "  #             calculate the entropy of the \"less\" list\n",
        "            less_entropy = entropy(val_less_than_theta)\n",
        "  #             calculate the entropy of the \"greater\" list\n",
        "            greater_entropy = entropy(val_greater_than_theta)\n",
        "  #             calculate the info gain using the formular.\n",
        "  #             Entropy(Dataset) – (Count(Group1) / Count(Dataset) * Entropy(Group1) + Count(Group2) / Count(Dataset) * Entropy(Group2))\n",
        "            info_gain = entropy_feature_values - (((len(idx_less_than_theta)/len(class_labels))*less_entropy)+((len(idx_greater_than_theta)/len(class_labels))*greater_entropy))\n",
        "            \n",
        "  #             if current info gain > max info gan\n",
        "            if info_gain > max_info_gain:\n",
        "  #                 update the info gain, \n",
        "                max_info_gain = info_gain\n",
        "                theta = current_theta\n",
        "  #                     the theta, the best index list of right, \n",
        "                best_idx_left = idx_less_than_theta\n",
        "                best_idx_right = idx_greater_than_theta\n",
        "  #                         the best index list of left and class_labels_list_after_split\n",
        "                class_labels_split = val_less_than_theta + val_greater_than_theta\n",
        "\n",
        "#         return the max info gain, theata,the best left list,the best right list and class label after split\n",
        "        return max_info_gain, theta, best_idx_left, best_idx_right, class_labels_split\n",
        "\n",
        "    #method to select the best feature out of all the features.\n",
        "    def best_feature(self,dict_rep):\n",
        "#         set key value to none\n",
        "        key_value = None\n",
        "#         set best info gain to -1\n",
        "        best_info_gain = -1\n",
        "#         set best theta to 0\n",
        "        best_theta = 0\n",
        "#         set best left list to empty\n",
        "        best_left_list = []\n",
        "#         set best right list to empty\n",
        "        best_right_list = []\n",
        "#         set best class labels after split to empty\n",
        "        best_class_labels = []\n",
        "#         init a result list\n",
        "        result = []\n",
        "#         for each key in dict_rep:\n",
        "        for ke in dict_rep.keys():\n",
        "#             using cal_best_theta_value() and store all returned values\n",
        "            max_info_gain = self.cal_best_theta_value(ke,dict_rep[ke])[0]\n",
        "            theta = self.cal_best_theta_value(ke,dict_rep[ke])[1]\n",
        "            idx_left = self.cal_best_theta_value(ke,dict_rep[ke])[2]\n",
        "            idx_right = self.cal_best_theta_value(ke,dict_rep[ke])[3]\n",
        "            class_labels_list_after_split = self.cal_best_theta_value(ke,dict_rep[ke])[4] \n",
        "#             if info gian is greater than best info gain:\n",
        "            if max_info_gain > best_info_gain:\n",
        "#                 update info gain, theth, key value,\n",
        "                best_info_gain = max_info_gain\n",
        "                best_theta = theta\n",
        "                key_value = ke\n",
        "  \n",
        "#                     left list, right list, class labels after split\n",
        "                best_left_list = idx_left\n",
        "                best_right_list = idx_right\n",
        "                best_class_labels = class_labels_list_after_split\n",
        "\n",
        "#         append the key value to the retrun list\n",
        "        result.append(key_value)\n",
        "        # result.append(best_info_gain)\n",
        "\n",
        "#         append the theta value to the retrun list\n",
        "        result.append(best_theta)\n",
        "\n",
        "#         append the left list to the retrun list\n",
        "        result.append(best_left_list)\n",
        "\n",
        "#         append the right list to the retrun list\n",
        "        result.append(best_right_list)\n",
        "\n",
        "#         append the class labels to the retrun list\n",
        "        result.append(best_class_labels)\n",
        "\n",
        "#         return the list.\n",
        "        #print(result)\n",
        "        return  result\n",
        "\n",
        "    def get_remainder_dict(self,dict_of_everything,index_split):\n",
        "        global fea_list\n",
        "#         init a split dict\n",
        "        split_dict = {}\n",
        "#         for each key \"ke\" in dict_of_everything:\n",
        "        for ke in dict_of_everything.keys():\n",
        "#             init a value list\n",
        "            value_list = []\n",
        "#             init a modified list\n",
        "            modified = []\n",
        "#             get the corresponding values of the key\"ke\" \n",
        "            values_of_ke = dict_of_everything[ke]\n",
        "#             for each value and its corresponding index of the key\"ke\" \n",
        "            for idx in range(len(values_of_ke)):\n",
        "#                 if index is not in the index_split:\n",
        "                if idx not in index_split:\n",
        "#                     append it to the modified list and value list\n",
        "                    modified.append(values_of_ke[idx])\n",
        "                    value_list.append(values_of_ke[idx][1])\n",
        "#             add this modified list to the dict\n",
        "            split_dict[ke] = modified\n",
        "#         return the splited dict and val list\n",
        "        return split_dict, value_list\n",
        "\n",
        "    #method to create decision tree\n",
        "    def create_decision_tree(self, dict_of_everything,class_val,eta_min_val):\n",
        "        global fea_list\n",
        "        #if all the class labels are same, then we are set\n",
        "        if len(set(class_val)) == 1:\n",
        "            #print(\"Leaf node for set class is\",class_val[0],len(class_val))\n",
        "            return Node(class_val[0], None, None, None, True)\n",
        "        #if the no class vales are less than threshold, we assign the class with max values as the class label    \n",
        "        elif len(class_val) < eta_min_val:\n",
        "            return Node(self.cal_major_class_values(class_val), None, None, None, True)\n",
        "#         else:\n",
        "        else:\n",
        "#             using the best_feature to get best feature list\n",
        "            best_feature_list = self.best_feature(dict_of_everything)\n",
        "#             store the node name, theta, left split, right split and class labes\n",
        "            node_name = best_feature_list[0]\n",
        "            theta = best_feature_list[1]\n",
        "            left_split = best_feature_list[2]\n",
        "\n",
        "            right_split = best_feature_list[3]\n",
        "            \n",
        "            class_label = best_feature_list[4]\n",
        "\n",
        "#             call get_remainder_dict to get left tree data\n",
        "            # left_dict = self.get_remainder_dict(dict_of_everything, left_split)[0]\n",
        "            # left_class = self.get_remainder_dict(dict_of_everything, left_split)[1]\n",
        "            left_dict, left_class = self.get_remainder_dict(dict_of_everything, left_split)\n",
        "\n",
        "#             call get_remainder_dict to get right tree data\n",
        "            # right_dict = self.get_remainder_dict(dict_of_everything, right_split)[0]\n",
        "            # right_class = self.get_remainder_dict(dict_of_everything, right_split)[1]\n",
        "            right_dict, right_class = self.get_remainder_dict(dict_of_everything, right_split)\n",
        "\n",
        "#             call create_decision_tree to get left tree based on the left tree data\n",
        "            left_tree = self.create_decision_tree(left_dict, left_class, eta_min_val)\n",
        "\n",
        "#             call create_decision_tree to get right tree based on therightleft tree data\n",
        "            right_tree = self.create_decision_tree(right_dict, right_class, eta_min_val)\n",
        "\n",
        "        \n",
        "#             set the root node\n",
        "            root_node =  Node(node_name, right_tree, left_tree, theta, False)\n",
        "            \n",
        "#             return root node\n",
        "            return root_node\n",
        "            \n",
        "    #fit the decisin tree\n",
        "    def fit(self, dict_of_everything,cl_val,features,eta_min_val):\n",
        "#         set the fea_list the value of features\n",
        "        global fea_list\n",
        "        fea_list = features\n",
        "#         set the root node using the function create_decision_tree()\n",
        "        self.root_node = self.create_decision_tree(dict_of_everything, cl_val, eta_min_val)\n",
        "        return self.root_node\n",
        "\n",
        "    def classify(self,row,root):\n",
        "#         init the test dict\n",
        "        dic = {}\n",
        "#         add row to the dict\n",
        "        for k,j in enumerate(row):\n",
        "            dic[k] = j\n",
        "#         set the current node to root\n",
        "        current_node = root\n",
        "#         while the current node is not leaf:\n",
        "        while not current_node.leaf:\n",
        "#             implement the case whether the current shoud go to the left\n",
        "            if float(dic[current_node.root_value]) <= float(current_node.theta):\n",
        "                current_node  = current_node.root_left\n",
        "#             implement the case whether the current shoud go to the right\n",
        "            else:\n",
        "              current_node = current_node.root_right\n",
        "#         return the calss of the current node\n",
        "        return current_node.root_value\n",
        "        \n",
        "    # method to the labels for the test data\n",
        "    def predict(self, X, root):\n",
        "#         predict using the classify()\n",
        "        l = [self.classify(row,root) for row in X]\n",
        "        return l\n",
        "\n",
        "# D1 = DecisionTree()\n",
        "# dic_rep, feature_list = build_dict_of_attributes_with_class_values(X,y)\n",
        "# class_values = y\n",
        "# eta_min_val = 10\n",
        "# D1.create_decision_tree(dic_rep, class_values, eta_min_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccedAQfJaTIj"
      },
      "source": [
        "def accuracy_for_predicted_values(test_class_names1,l):\n",
        "#     init true and false count to 0\n",
        "    true, false = 0, 0\n",
        "#     for each prediction,if predict is correct then, true++ else, false++\n",
        "    for i in range(len(test_class_names1)):\n",
        "        if test_class_names1[i] == l[i]:\n",
        "            true += 1\n",
        "        else:\n",
        "            false += 1\n",
        "    acc = true / (true + false)\n",
        "#     return the acc\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhe31j_1UK1b"
      },
      "source": [
        "def main(num_arr, eta_min):\n",
        "    eta_min_val = round(eta_min*num_arr.shape[0])\n",
        "\n",
        "    #randomly shuffle the array so that we can divide the data into test/training\n",
        "    num_arr = random_numpy_array(num_arr)\n",
        "    \n",
        "    #divide data into ztest labels,test features,training labels, training features\n",
        "    train_features, train_labels, test_features, test_labels = generate_set(num_arr)\n",
        "\n",
        "    #init cumulate acc to 0\n",
        "    cumulate_acc = 0\n",
        "\n",
        "    # ten fold iteration \n",
        "    # print(\"train_features: \",train_features)\n",
        "    # print(\"train_features[0]\",train_features[0])\n",
        "\n",
        "    for i in range(10):\n",
        "        #build a dictionary with class labels and respective features values belonging to that class\n",
        "        attri_dict, feature_list = build_dict_of_attributes_with_class_values(train_features[i], train_labels[i])\n",
        "        \n",
        "        #instantiate decision tree instance\n",
        "        dTree = DecisionTree()\n",
        "        \n",
        "        # build the decision tree model.\n",
        "        dTree.fit(attri_dict, train_labels[i], feature_list, eta_min_val)\n",
        "        \n",
        "        #predict the class labels for test features\n",
        "        #print(\"root_node: \", dTree.root_node)\n",
        "        predict_labels = dTree.predict(test_features[i], dTree.root_node)\n",
        "        #print(\"predict_labels: \", predict_labels)\n",
        "        \n",
        "        #calculate the accuracy for the predicted values\n",
        "        acc = accuracy_for_predicted_values(predict_labels, test_labels[i])\n",
        "        \n",
        "        #add acc to cumulate acc\n",
        "        cumulate_acc += acc\n",
        "\n",
        "        print(\"Accuracy: \", accuracy_for_predicted_values(predict_labels, test_labels[i]))\n",
        "    print(\"Accuracy across 10-cross validation for\", eta_min, \"is\", float(cumulate_acc)/10, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azsc28ooaTIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c29d80-78fa-44b9-c445-c743a671716e"
      },
      "source": [
        "eta_min_list = [0.05,0.10,0.15,0.20]\n",
        "newfile = \"iris.csv\"\n",
        "num_arr = load_csv(newfile)\n",
        "for i in eta_min_list:\n",
        "  main(num_arr,i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  1.0\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  1.0\n",
            "Accuracy:  1.0\n",
            "Accuracy across 10-cross validation for 0.05 is 0.9400000000000001 \n",
            "\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.8\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  1.0\n",
            "Accuracy across 10-cross validation for 0.1 is 0.9266666666666667 \n",
            "\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  0.8\n",
            "Accuracy:  1.0\n",
            "Accuracy:  1.0\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  1.0\n",
            "Accuracy across 10-cross validation for 0.15 is 0.9266666666666665 \n",
            "\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  0.9333333333333333\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  1.0\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy:  0.8666666666666667\n",
            "Accuracy across 10-cross validation for 0.2 is 0.9266666666666667 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Oq5-mUwaTIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1f61be-ed3f-4069-c7fc-536594f87edf"
      },
      "source": [
        "eta_min_list = [0.05,0.10,0.15,0.20,0.25]\n",
        "newfile = \"spambase.csv\"\n",
        "num_arr = load_csv2(newfile)\n",
        "for i in eta_min_list:\n",
        "  main(num_arr,i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.975\n",
            "Accuracy:  0.875\n",
            "Accuracy:  0.975\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.9\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.875\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.875\n",
            "Accuracy across 10-cross validation for 0.05 is 0.925 \n",
            "\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.875\n",
            "Accuracy:  0.9\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.875\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.825\n",
            "Accuracy:  0.975\n",
            "Accuracy:  0.9\n",
            "Accuracy across 10-cross validation for 0.1 is 0.9100000000000001 \n",
            "\n",
            "Accuracy:  0.9\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.875\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.975\n",
            "Accuracy:  0.9\n",
            "Accuracy across 10-cross validation for 0.15 is 0.9225 \n",
            "\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.95\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.9\n",
            "Accuracy:  0.8\n",
            "Accuracy:  0.875\n",
            "Accuracy:  0.975\n",
            "Accuracy:  0.95\n",
            "Accuracy across 10-cross validation for 0.2 is 0.9225 \n",
            "\n",
            "Accuracy:  0.975\n",
            "Accuracy:  0.875\n",
            "Accuracy:  0.925\n",
            "Accuracy:  0.975\n",
            "Accuracy:  0.9\n",
            "Accuracy:  0.975\n",
            "Accuracy:  0.9\n",
            "Accuracy:  1.0\n",
            "Accuracy:  0.9\n",
            "Accuracy:  0.875\n",
            "Accuracy across 10-cross validation for 0.25 is 0.93 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}